name=Commit AI
settings.title=Settings
settings.general.group.title=Commit AI
settings.locale=Locale
settings.locale.contextHelp=Listed locales appear in the IDE's language. Prompts use English locales (e.g., German, French, etc.).
settings.prompt=Prompt
settings.github-star=Star on GitHub
settings.report-bug=Report bug
settings.kofi=Buy me a coffee
settings.github-sponsors=Sponsor me
settings.verifyToken=Verify
settings.verify.valid=Configuration is valid.
settings.verify.invalid=Invalid configuration: {0}.
settings.verify.running=Verifying configuration...
settings.refreshModels.success=Models have been refreshed successfully.
settings.refreshModels.running=Refreshing models....
settings.refreshModels=Refresh
unknown-error=Unknown error
action.background=Generating a commit message
notifications.title=Commit AI
notifications.welcome=Thanks for installing Commit AI <strong>{0}</strong>
notifications.unsuccessful-request=Error occurred: {0}
notification.group.important.name=Commit AI important
notification.group.general.name=Commit AI general
notifications.no-common-branch=Branch could not be found. Replacing {branch} with main.
notifications.client-not-set=LLM client is not set.
actions.do-not-ask-again=Do not ask me again.
actions.take-me-there=Take me there.
actions.sure-take-me-there=Sure, take me there.
notifications.prompt-too-large=The diff is too large for the OpenAI API. Try reducing the number of staged changes, or write your own commit message.
notifications.no-commit-message=Commit field has not been initialized correctly.
notifications.unable-to-save-token=Token could not be saved: {0}.
settings.prompt.name=Name
settings.prompt.content=Content
validation.required=This value is required.
validation.integer=Value should be an integer.
validation.float=Value should be a float.
validation.double=Value should be double.
validation.temperature=The temperature should be between 0 and 2.
settings.prompt.comment=<ul>\n  <li>Customize your prompt with variables: {locale}, {diff}, {branch}, {hint}, {taskId}, {taskSummary}, {taskDescription} and {taskTimeSpent}.</li>\n  <li>Include a hint by using a dollar sign prefix like this: {Here is a hint: $hint}. This adds the hint text inside the curly brackets only if a hint is provided.</li>\n  <li>Note: The prompt preview displays only the first 10,000 characters.</li>\n </ul>
actions.update=Update
actions.add=Add
actions.reset=Reset to Default
settings.prompt.edit.title=Edit Prompt
settings.prompt.add.title=Add Prompt
settings.prompt.description=Description
settings.prompt.projectSpecific=Project specific
settings.prompt.projectSpecific.contextHelp=When selected, saves the chosen prompt and locale for this project only.
validation.unique=Value already exists.
settings.more-prompts=More prompts
settings.more-llm-clients=More LLM client free
settings.exclusions.group.title=Exclusions
settings.exclusions.app.exclusion=Exclusion glob
settings.exclusions.app.dialog.label=Enter the new path glob ('*' and '?' allowed):
settings.exclusions.app.dialog.title=Add Exclusion Glob
settings.exclusions.app.title=Global exclusions
settings.exclusions.project.title=Project exclusions
settings.prompt.hint=Hint
settings.prompt.hint.comment=Note: This field is for testing purposes only. When generating the actual prompt, the {hint} variable will be replaced with content from the commit dialog.
settings.llmClient=LLM Client
settings.llmClient.projectSpecific=Project specific
settings.llmClient.projectSpecific.contextHelp=When selected, saves the chosen LLM client for this project only.
settings.llmClient.name=Name
settings.llmClient.host=Host
settings.llmClient.token=Token
settings.llmClient.token.stored=<hidden>
settings.llmClient.modelId=Model
settings.llmClient.modelId.comment=This is an editable combo box, which means you can write your own model IDs, and they will be added to the dropdown.
settings.llmClient.timeout=Timeout
settings.llmClient.temperature=Temperature
settings.llmClient.temperature.comment=What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make \
the output more random, while lower values like 0.2 will make it more focused and deterministic.
settings.llmClient.streamingResponse=Streaming response
settings.llmClient.streamingResponse.contextHelp=Some models do not support streaming response and will fall back to normal response.
settings.llmClient.topK=Top K
settings.llmClient.topK.comment=The Top K parameter changes how the model selects tokens for output. \
  A Top K of 1 means the selected token is the most probable among all the tokens in the model\'s vocabulary (also called greedy decoding), \
  while a Top K of 3 means that the next token is selected from among the 3 most probable using the temperature. \
  For each token selection step, the Top K tokens with the highest probabilities are sampled. \
  Tokens are then further filtered based on topP with the final token selected using temperature sampling.
settings.llmClient.topP=Top P
settings.llmClient.topP.comment=The Top P parameter changes how the model selects tokens for output. \
  Tokens are selected from the most to least probable until the sum of their probabilities equals the Top P value. \
  For example, if tokens A, B, and C have a probability of 0.3, 0.2, and 0.1 and the Top P value is 0.5, \
  then the model will select either A or B as the next token by using the temperature and exclude C as a candidate.